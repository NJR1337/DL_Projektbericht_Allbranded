\chapter{Architektur}


\section{EDA}

\subsection{Antwortsnippets}

% Ein Bestandteil des Projekts war die Analyse von
% Intention-Kategorien. Ziel war es, aus einer Vielzahl von Antwortsnippets
% aus Customer-Support-Templates zentrale Kategorien wie „Lieferstatus“ oder
% „Expressversand“ zu extrahieren. Hierfür wurde eine Verteilungsanalyse durchgeführt,
% bei der häufige Fragen der Kunden identifiziert und in relevante Themenbereiche
% eingeordnet wurden. Die Aggregation der Antwortsnippets und die Abschätzung
% benötigter Intentionen halfen, eine klare Struktur zu schaffen und eine fundierte
% Basis für die weitere Verarbeitung zu legen. 

Ein  Bestandteil des Projekts war die Analyse von Intention-Kategorien, bzw. der E-Mail-Anfragen 
die das Customer Support die zu bearbeiten hatte. Ziel war es, aus einer Vielzahl von Antwortvorschlägen 
(oder Antwortsnippets) aus dem Customer-Support-Templates zentrale Kategorien wie „Lieferstatus“ oder 
„Expressversand“ zu extrahieren. Parallel standen uns ca. 110 Beispiel-Mails zur Verfügung, aus der wir 
eine Verteilung der Anfragen abgeleitet haben. Die Verteilungsanalyse wurde benutzt um häufige Fragen 
der Kunden zu identifizieren und in relevante Themenbereiche zu erstellen, um die Antwortvorschläge zu 
aggregieren. Die Aggregation der Antwortsnippets und die Abschätzung benötigter Intentionen halfen, 
eine klare Struktur zu schaffen und eine fundierte Basis für die weitere Verarbeitung zu legen.

\subsection{Erkennung der Anrede}

Gemäß der mit dem Kunden vereinbarten Kommunikationsform erfolgt die Unterscheidung
zwischen Du- und Sie-Ansprache. Dementsprechend existieren für beide Formen separate
Antwortsnippets. Für die korrekte Klassifikation der Anrede verwendet das System das
Skript du\textunderscore labeling.py . Dieses Script analysiert den Inhalt der Kundenmail,
zählt die Anzahl an Wörtern, die für die Ansprache mit „du“ oder „Sie“ sprechen, und wählt
die passende Anrede. Bei gleicher Anzahl an Wörtern, wird “Sie” als Anrede ausgegeben. 

\section{Preprocessing}

\subsection{Data-Extraction}

Die E-Mails wurden von Allbranded in acht JSON-Dateien (jeweils 1000 Mails) bereitgestellt.
Zunächst wurden diejenigen E-Mails ausgeschlossen, die von Allbranded selbst verfasst
wurden. Das Skript mail\textunderscore extractor.py liest die Mails ein und speichert sie einzeln in
separaten JSON-Dateien. Erste Testmails enthielten eine Mailhistorie und zahlreiche
HTML-Tags, weshalb anfänglich Funktionen implementiert wurden, um anhand der Grußformel
sowie <span>-Tags die relevante E-Mail zu extrahieren und den übrigen Inhalt zu entfernen.
Mithilfe der Bibliothek BeautifulSoup erfolgte anschließend die Entfernung von restlichen
HTML-Tags. 

\subsection{Paraphrasierer}
% Parallel dazu wurde ein Paraphrasierer in die bestehende Projektstruktur integriert. Dieser nutzt
% moderne Methoden wie Backtranslation sowie T5- und MT5-Modelle, um Sätze inhaltlich
% umzuformulieren. Die Implementierung erwies sich als anspruchsvoll, da die Ergebnisse zunächst
% wenig sinnvoll waren und durch iterative Anpassungen verbessert werden mussten. Im Verlauf der
% Arbeit war es notwendig, die zugrunde liegenden NLP-Konzepte zu verstehen, um den
% Paraphrasierungsprozess zu optimieren. Der Paraphrasierer mit der zugehörigen main.py wurde
% so konzipiert, sodass ein Skript die bestehenden manuell gelabelten Daten je Kategorien
% zählt und gemessen wird wie viel Samples bis zu einer definierten Anzahl fehlen.
% Das Hauptskript nutzt diese und die Methoden des Paraphrasieres, und ermöglicht
% über eine manuelle Auswahl einer Intent oder Sentimentkategorie paraphrasierte Mails zu erzeugen. 

Unter der Annahme, dass das Labeln der Daten und die daraus resultierende Menge an Beispielen 
je Kategorie nicht ausreichend sein wird, wurde ein Paraphrasierer in die bestehende Projektstruktur 
integriert, der auf einem fundierten Verständnis der zugrunde liegenden NLP-Konzepte basiert. 
Der Paraphrasierer kombiniert drei Ansätze zur inhaltlichen Umformulierung von Sätzen: 
Er nutzt einerseits eine Fill-Mask-Substitution mittels des vortrainierten Modells 
„bert-base-german-cased“, andererseits eine T5-basierte Paraphrasierung mit dem Modell 
„seduerr/t5\textunderscore base\textunderscore paws\textunderscore ger“ und dem „t5-base“-Tokenizer sowie eine Pivot-Übersetzung unter 
Verwendung lokaler MarianMT-Modelle, die als Backtranslation dienen. Die Implementierung erwies sich 
als anspruchsvoll, da die initialen Ergebnisse wenig überzeugend waren und erst durch iterative 
Anpassungen optimiert werden konnten. Das zugehörige Hauptskript zählt die existierenden manuell 
gelabelten Daten je Kategorie, ermittelt den Bedarf an zusätzlichen Samples und ermöglicht über eine 
manuelle Auswahl einer Intent- oder Sentimentkategorie die Erzeugung paraphrasierter Mails, 
die anschließend als neue Trainingsbeispiele in das System integriert werden.

\section{Labelling}

\subsection{Label-Studio}

Label-Studio ist ein Open-Source-Labelling-Tool mit umfangreichen Funktionen,
wie vorgefertigten Label-Templates und Workflows für unterschiedliche Datentypen
wie z.B. Texte und Bilder und unterschiedlichen Verwaltungsmöglichkeiten von Labeln
und Usern im Projekt. Zu Beginn des Projektes wurde dies für den Einsatz in Betracht
gezogen, jedoch aufgrund unser gerningen Anforderungen an das Tool  verworfen und sich für
eine eigene Labelling Lösung entschieden, welche im folgenden Abschnitt näher erläutert wird.  

\subsection{Labeling Skript}
Für das Labeling der Mails auf dem Server wurde das Skript data\textunderscore labeling.py
entwickelt. Das Skript fordert zunächst eine entsprechende Ordnerauswahl an, um ein
paralleles Labeling ohne Überschneidungen zu ermöglichen. Anschließend wird der Inhalt einer
ausgewählten Mail in der Konsole zusammen mit den möglichen Klassifikationsoptionen
angezeigt. Nach Eingabe einer gültigen Klassifikation des Intents und des Sentiments
wird die JSON-Datei in den klassenspezifischen Sentiment- und Intent-Unterordnern
gespeichert. Diese Vorgehensweise dient der besseren Übersicht über die Klassenverteilung
und dem einfachen Laden der Daten in die Modelle. 

\subsection{Sample Balancing}

Ein zentrales Problem im Projekt war das Sample-Balancing. Viele Kategorien waren stark
unterrepräsentiert, wodurch das Training des Modells erschwert wurde. In einem ersten Schritt wurde
der Dataloader ohne jegliche Korrektur getestet, was zu unausgewogenen Ergebnissen führte. Danach
wurden verschiedene Techniken ausprobiert: Undersampling, bei dem die Kategorie mit der niedrigsten
Verfügbarkeit als Grenze gesetzt wurde, und Oversampling mittels eines Weighted Random Samplers, der
für ausgeglichene Batches im Training sorgte. Diese Methode erzielte letztlich die besten Ergebnisse
und bildete die Grundlage für die weitere Optimierung des Sentiment-Modells. Zusätzlich wurde ein
„augmented“-Modus eingeführt, bei dem Kategorien mit wenigen Samples durch generierte Daten ergänzt
wurden, unter Verwendung des zuvor entwickelten Paraphrasierers.

\subsection{Self-Training}

Aufgrund der Problematik der ungelabelten Trainingsdaten wurde der Self-Training Ansatz
untersucht um den Aufwand für das Labeln zu senken. Self-Training gehört zum Semi-Supervised
Learning und bei dem zu Beginn ein Modell auf einem kleinen Datensatz Trainiert wird. Die
erlernten Labels soll das Modell dann auf die ungelabelten Daten übertragen. Die Pseudo-Labels
des Modells werden nur übernommen und dem Trainingsdatensaz hinzugefügt, wenn sie einen
Schwellwert übersteigen. Dieser Ablauf wird solange iterativ durchgeführt, bis ein ausreichend
großer Datensatz für das Training eines großen Modells vorhanden ist.  

Aufgrund der geringen Anzahl an Trainingsdaten und der unbalancierten Klassen wurden keine 
verwendbaren Labels erzeugt, weshalb sich auf Few-Shot Learning und Parahprasing fokussiert wurde. 

\newpage
\section{Modelltraining}

\subsection{Allgemeines}

Die Entwicklung des Intent- und Sentiment-Modells folgte einem systematischen Vorge- hen, 
das auf bewährten Machine-Learning-Methoden basierte. Beide Modelle wurden zunächst unter 
Verwendung der Cookie-Cutter-Struktur organisiert, die eine klare Trennung von Daten, Skripten 
und Modellkonfigurationen ermöglicht. Anschließend kam BERT (Bidirectional Encoder Representations 
from Transformers) als zentraler Modellkern zum Einsatz. Hierbei wurde Transfer Learning angewandt: 
Anstatt das Modell von Grund auf neu zu trainieren, griffen wir auf vortrainierte BERT-Modelle von 
huggingface zurück und führten Fine-Tunings durch, um die Modelle für die spezifischen Aufgaben der 
Intent- und Sentimentklassifikation zu verwenden.

Um die trainierten Modelle zu testen, wurden mehrere Metrics-Dateien erstellt, welche die zuvor 
gespeicherten Modelle laden und die Performance auf den Testdaten messen. Zu Messung der Performance 
wurde der F1-Score, zusammengesetzt aus Precision und Recall herangezogen. Der Score wurde dabei für 
alle Testdaten ermittelt, und zusätzlich für die einzelnen Kategorien, was uns weitere interessante 
Einblicke in die Performance unserer Modelle lieferte.
 

\subsection{Sentiment}
\subsubsection{BERT}

Für die Sentiment-Analyse wurde ein eigenes Modell entwickelt, das auf einer modularen
Architektur basiert. Zu den wesentlichen Bestandteilen gehörten unter anderem separate
Dateien für das Modell selbst, den Datenladeprozess, Metriken und das Haupttrainingsskript.
Durch den Einsatz eines Weighted Random Samplers, Dropout, adaptiver Lernraten und des
Layer-Freezings konnten wir die Trainingsqualität erheblich verbessern. Die
Hyperparameter-Optimierung, realisiert durch Grid- und Random-Search-Methoden, war eine
Herausforderung, insbesondere aufgrund der technischen Gegebenheiten des Servers. Ein
frühzeitiges Abbruchkriterium (Early Stopping) basierend auf der Validierungs-Loss wurde
eingeführt, um übermäßiges Overfitting zu vermeiden. Letztlich zeigte sich, dass die Qualität
der Daten einen entscheidenden Einfluss auf die Ergebnisse hatte. Das Problem der Datenimbalance
war ein zentraler Punkt, der schließlich durch Sample-Balancing-Strategien adressiert wurde. 


\subsubsection{SVM/BERT}

Aufgrund der zuvor beschriebenen Thematik der unbalancierten Klassen in den Sentiment und
Intent Daten wurde der Ansatz BERT+SVM als Vergleich herangezogen. Die Beiden Ansätze wurden
in Kombination verwendet, wobei die druch den BERT-Transformer generierten Embeddings als Input
für die Support-Vektor-Machine (SVM) dienen. Es hat sich aber herausgestellt, dass der Ansatz
torzt der ergänzung von Methoden wie PCA und Under- und Oversampling und verschiedenen
Hyperparameterkombinationen der SVM zu keinen besseren Ergebnissen als das Finetuning eines
Vortrainierten BERT-Transformers führt.  

\subsubsection{Logistische Regression}

Die logistische Regression benötigt numerische Features, weshalb zur Umwandlung der 
Textdaten die Methode Term Frequency-Inverse Document Frequency (TF-IDF) verwendet wurde. Diese Methode 
berücksichtigt sowohl die Häufigkeit eines Wortes als auch dessen relative Seltenheit. Für jedes Sentiment 
wird eine lineare Kombination der numerischen Feature-Werte mit spezifischer Gewichtung berechnet.
 Anschließend werden die berechneten Werte mittels einer Softmax-Funktion in Konfidenzwerte umgerechnet,
  welche für die Klassenvorhersage genutzt werden. Das Modell ermöglicht ein schnelles Training und eine rasche Klassifizierung,
 zeigte jedoch im Vergleich zu Ansätzen mit Transformern schlechtere Ergebnisse. 

\subsubsection{Few-Shot Learning}

Für die Entwicklung eines Fewshot-Learning-Modells wurde SetFit eingesetzt. Ausgehend von
50 bereinigten Datensamples pro Kategorie wurden verschiedene Szenarien getestet, wobei sich
die besten Ergebnisse mit jeweils 10 Samples pro Kategorie ergaben. SetFit verwendet
Satz-Embeddings auf Basis von SBERT und bildet Paare von Samples, um Cosinus-Distanzen zu
berechnen. Über diese Distanzmaße erlernt das Modell Wahrscheinlichkeiten und klassifiziert so
neue Datenpunkte. Allerdings zeigte sich, dass die Ergebnisse nicht an die Qualität des
BERT-Ansatzes mit Weighted Random Sampler heranreichten.

Allerdings konnte der Ansatz dafür benutzt werden, die bisher gelabelten Daten präsziser zu Labeln. 
Hierzu wurden zunächst je zehn Mails pro Klasse exemplarisch ausgewertet, wobei jeweils das vorhergesagte 
Label, das tatsächliche Label und die zugehörige Konfidenz des Modells angezeigt wurden. Aufgrund des 
Trainings auf sauber gelabelten Daten konnte das Modell abweichende Labels gut erkennen und Diskrepanzen 
zwischen tatsächlich zugewiesenem Label und Modellvorhersage identifizieren. Aufgrund der guten Ergebnisse
wurde das Few Shot Learning Modell genutzt, um die unsauber gelabelten E-Mails zu erkennen. 
Es wurden Dateien aussortiert, welche ein abweichendes Label oder eine Konfidenz von unter 70 \% haben.   
Auf dem neu entstandenen Datensatz wurde das Sentiment Modell erneut trainiert und getestet. Es 
konnte eine signifikante Verbesserung erzielt werden. Es muss jedoch bedacht werden, dass Mails die 
uneindeutig formuliert wurden, auch im Vorhinein aussortiert wurden. Dadurch wird die Erkennung auf den 
Testdaten vereinfacht. 
 
\subsubsection{Modellergebnisse}
In der folgenden Tabelle sind einmal übersichtlich die besten Modelle der oben aufgeführten
Ansätze für die Sentimentklassifikation dargestellt. 

\begin{table}[H]
\hspace*{-1cm}
\centering
\begin{tabular}{c|c|c|c|c|c}
        \toprule
         \textbf{Modell} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Anmerkungen} \\
         \midrule
         \textbf{BERT (Basic)} & 0.8223 & 0.8824 & 0.8223 & 0.8404 & Ungesäuberte Daten \\
         \textbf{BERT Few-Shot} & 0.6641& 0.8396 & 0.6641 & 0.7199 & Ungesäuberte Daten \\
         \textbf{BERT Sentiment} & 0.8386 & 0.9468 & 0.8336 & 0.8820 & Ungesäuberte Daten \\
         \textbf{BERT + SVM} & 0.86 & 0.822 & 0.859 & 0.826 & Ungesäuberte Daten\\
         \textbf{Logistische Regression} & 0.7962& 0.8243 & 0.7962 & 0.8084 & Ungesäuberte Daten\\
         \midrule
         \textbf{BERT Few-Shot} & 0.9123& 0.9354 & 0.9123 & 0.9173 & Gesäuberte Daten \\
         \textbf{BERT Few-Shot} & 0.9373 & 0.9476 & 0.9373 & 0.9401 & Gesäuberte Daten \\
         \bottomrule
\end{tabular}
\caption{Übersicht Modellergebnisse Sentiment}
\label{tab:Modellergebnisse Sentiment}
\end{table}


\subsection{Intent} 
Für die Intent-Analyse wurde derselbe Ansatz wie bei der Sentiment-Analyse gewählt. 
Grundlegende Techniken wie Weighted Random Sampling, Dropout, adaptive Lernraten- anpassung 
und Layer-Freezing kamen zum Einsatz, um das Modell zu optimieren. Zwei unterschiedliche 
Ausgangsmodelle wurden untersucht: Zum einen das BERT-German- Uncased-Modell, das speziell 
für die Klassifikation deutscher Dokumente konzipiert ist, und zum anderen DistilBERT, 
ein kleineres Modell, das getestet wurde, um zu schauen wie die Performance bei einem 
ressourcenschonende Modell ist. Die besten Ergebnisse erzielte das BERT-German-Uncased-Modell. 

Als weitere Fine Tuning Technik wurde LoRA (Low-Rank Adaptation) eingesetzt, das eine 
ressourcenschonende Anpassung von vortrainierten Modellen ermöglicht. Die Hauptgewichtungen 
eines vortrainierten Modells wird dabei eingefroren und stattdessen trainierbare, nieder-rangige Matrizen 
eingefügt. Dadurch werden nur wenige Parameter angepasst, was einen ressourcenschonenden und effizientes 
Fine Tuning ermöglicht. Der Einsatz von LoRA führte jedoch nicht zu einer signifikanten 
Leistungsverbesserung. Aufgrund von Zeitbeschränkungen wurde der Großteil der Evaluierungen zunächst 
im Bereich der Sentiment-Erkennung durchgeführt, sodass der Fokus bei der Intent-Erkennung etwas 
zurückgestellt werden musste. 

Die Erkennungsgenauigkeit des Modells variiert stark je nach Klasse. Klassen mit häufig verwendeten 
Begriffen und einer großen Anzahl von Beispielen, wie etwa die Freigabe einer Druckskizze, 
wurden deutlich präziser klassifiziert als Intents mit sehr heterogenen Beispielen. 
Ein wesentlicher Problempunkt stellt zudem eine ungenaue Labelvergabe dar, welche durch 
fehlenden Kontext und unterschiedliche Interpretationen der Klassen entsteht. Des Weiteren können 
Mails auch mehrere Intents thematisieren. Es kann jedoch nur ein Label vergeben werden. 
Zukünftig könnte ein Few-Shot-Learning-Ansatz zur Datenbereinigung, ähnlich wie bei der 
Sentiment-Analyse, zur Verbesserung der Qualität der Labels beitragen. 

\subsubsection{Modellergebnisse}
In der folgenden Tabelle sind einmal die beiden besten Modelle für die Intentklassifikation
aufgeführt. 

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
            \toprule
             \textbf{Modell} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
             \midrule
             \textbf{BERT (bert-base-uncased)} & 0.5920 & 0.6170 & 0.5920 & 0.5527\\
             \textbf{distilbert-base-uncased} & 0.5946 & 0.6044 & 0.5946 & 0.5853  \\
             \bottomrule
\end{tabular}
\caption{Übersicht Modellergebnisse Intent}
\label{tab:Modellergebnisse Intent}
\end{table}


\newpage
\section{Schnittstelle und Webapp}

\subsection{Schnittstelle}

Im Rahmen unseres Projekts wurde die Schnittstelle für das Machine-Learning-Modell als REST-API implementiert.
Connexion Flask übernahm dabei das Path-Handling sowie die Verarbeitung von Requests und Responses, was eine
nahtlose Einbindung des Modells in unsere Anwendung ermöglichte. Diese Vorgabe des Auftraggebers brachte
allerdings einige Herausforderungen mit sich, insbesondere bei der Versionierung, der korrekten Angabe
von Datentypen und der Verknüpfung von App-Funktionen mit der API. Ein grundlegendes Verständnis von
REST-API-Konzepten und den spezifischen Mechanismen von Connexion Flask war entscheidend, um diese Probleme
zu lösen. 

Die Containerisierung der Schnittstelle wurde mittels Docker umgesetzt. Dafür wurde ein
Dockerfile auf Basis eines miniconda Docke-Image erstellt, wobei das entsprechende conda
Environment vom Server exportiert und im Container genutzt wird. Die Umsetzung erfolgte mithilfe
von Docker-Compose, welches zwei eigenständige Services bereitstellt: 

Der erste Service namens mail\textunderscore analysis ist über die Anwendung app.py realisiert und erlaubt
das flexible Austauschen der verwendeten Modelle, beispielsweise um nachtrainierte Sentiment-
oder Intent-Modelle einzubinden. Der zweite Service gradio\textunderscore app stellt eine Weboberfläche bereit,
die über die Anwendung gradio\textunderscore app.py umgesetzt ist. Diese Webapp ist auf dem Server unter der
Adresse http://138.199.202.254:8000 erreichbar. Bei Nutzeranfragen greift die Webapp intern
auf den Service mail\textunderscore analysis zu, welcher die Verarbeitung der Anfragen mit den hinterlegten
Modellen übernimmt und anschließend die Ergebnisse auf der Weboberfläche darstellt. Auf die genauen Funktionen der 
Webapp wird im folgenden Abschnitt geanuer eingegangen. 

\subsection{Webapp}

Die Webapp für unseren Prototypen wurde mit der Python Library Gradio implementiert und besteht
aus drei Hauptseiten: einer Instruktionsseite, einer Klassifikationsseite und einer Seite für
die Human-Feedback-Loop. Die Instruktionsseite erklärt die Funktionsweise und Bedienung
der Webapp. Auf der Klassifikationsseite kann der Anwender eine zu klassifizierende E-Mail
eingeben, welche anschließend durch den Klassifizieren-Button eine Anfrage an die Schnittstelle
zur Intent- und Sentiment-Klassifikation sendet. Die Ergebnisse der Klassifikation durch die
Schnittstelle werden dem Nutzer angezeigt. Zusätzlich zu den Klassifizierten Intent und Sentiment
werden noch die zum Intent passenden vorgefertigten Antworten aus einem von Allbranded zur
Verfügung gestellten Dokument angezeigt, welche sich durch den Kopieren-Button in die
Zwischenablage kopieren lassen.  

Die Seite Human Feedback Loop bietet die Möglichkeit, bei Falschklassifikationen Korrekturen
manuell vorzunehmen. Die Anpassungen sind dabei auf einzelne Intents und Sentiments beschränkt,
um fehlerhafte oder unklare Nutzereingaben auszuschließen, da das Modell ausschließlich auf
Single-Intent-Daten trainiert wurde. Korrigierte Klassifikationen werden noch einmal zur
Kontrolle ausgegeben, während die entsprechenden E-Mails separat in der gleichen Struktur
wie die Trainingsdaten gespeichert werden. Dies ermöglicht ein einfaches Nachtrainieren des
Modells anhand der während des Betriebs gesammelten, korrigierten Daten. 

\section{Entwicklungsumgebung}

Für die Umsetzung des Projektes wurde uns von Allbranded ein Server zur Verfügung gestellt.
Der Server hat einen 8 Kern Prozessor und 16 GB RAM sowie eine Festplatte mit einer Kapazität von 150 GB.
Auf den Server haben wir über eine SSH-Verbindung zugegriffen.  

Die Entwicklungsarbeiten haben parallel auf dem Server sowie Lokal stattgefunden wobei das Modelltraining
aus Datenschutzrechtlichengründen ausschließlich auf dem druchgeführt wurde. Auf dem Server haben wir die
Skripte für das Preprocessing und das Modelltraining innerhalb einer Conda-Environment entwickelt. Sie
Skripte die Lokal entwickelt wurden haben wir über ein Git-Repository mit dem Server synchronisiert.  

Für das Betreiben der Schnittstelle und der Weboberfläche ist auf dem Server auch Docker installiert.